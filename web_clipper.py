import os
import time
import requests
from github import Github
import openai
from notion_client import Client
import telegram
import logging
from fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Header, Request, Body
import uvicorn
import shutil
from pathlib import Path
from fastapi.security import APIKeyHeader, HTTPBearer, HTTPAuthorizationCredentials
import secrets
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from config import CONFIG  # æ·»åŠ è¿™è¡Œåœ¨æ–‡ä»¶å¼€å¤´
from bs4 import BeautifulSoup  # æ·»åŠ åˆ°å¯¼å…¥éƒ¨åˆ†
import html2text
from contextlib import asynccontextmanager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è®¾ç½® httpx æ—¥å¿—çº§åˆ«ä¸º WARNINGï¼Œéšè—è¯·æ±‚æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)

# å®šä¹‰å…¨å±€å˜é‡
handler = None
UPLOAD_DIR = Path("uploads")

# é…ç½®é™åˆ¶
MAX_FILE_SIZE = CONFIG.get('max_file_size', 10 * 1024 * 1024)  # ä»é…ç½®ä¸­è·å–æœ€å¤§æ–‡ä»¶å¤§å°
ALLOWED_EXTENSIONS = set(CONFIG.get('allowed_extensions', ['.html', '.htm']))
API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=True)

# æ›¿æ¢åŸæ¥çš„ API_KEY_NAME å’Œ api_key_header
security = HTTPBearer()

# æ·»åŠ  lifespan å‡½æ•°å®šä¹‰
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    åº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
    """
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    global handler
    handler = WebClipperHandler(CONFIG)
    UPLOAD_DIR.mkdir(exist_ok=True)
    
    # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ API keyï¼Œç”Ÿæˆä¸€ä¸ª
    if 'api_key' not in CONFIG:
        CONFIG['api_key'] = secrets.token_urlsafe(32)
        logger.info(f"Generated new API key: {CONFIG['api_key']}")
    
    yield
    
    # å…³é—­æ—¶æ‰§è¡Œçš„æ¸…ç†ä»£ç ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
    if UPLOAD_DIR.exists():
        shutil.rmtree(UPLOAD_DIR)

# åˆ›å»ºåº”ç”¨å’Œé™é€Ÿå™¨
app = FastAPI(lifespan=lifespan)
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """éªŒè¯ Bearer ä»¤ç‰Œ"""
    token = credentials.credentials
    if token != CONFIG.get('api_key'):
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication token",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return token

def verify_file(file: UploadFile):
    """éªŒè¯æ–‡ä»¶"""
    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in ALLOWED_EXTENSIONS:
        raise HTTPException(
            status_code=400,
            detail=f"File type not allowed. Allowed types: {', '.join(ALLOWED_EXTENSIONS)}"
        )
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file.file.seek(0, 2)  # ç§»åˆ°æ–‡ä»¶æœ«å°¾
    size = file.file.tell()  # è·å–æ–‡ä»¶å¤§å°
    file.file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
    
    if size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"File too large. Maximum size allowed: {MAX_FILE_SIZE/1024/1024}MB"
        )

def parse_filename(filename):
    """ä»æ–‡ä»¶åè§£æURL
    filename format: {random_prefix}_url.html (å…¶ä¸­urlä¸­çš„/è¢«æ›¿æ¢ä¸º$)
    """
    try:
        # ç§»é™¤ .html åç¼€
        name_without_ext = filename.rsplit('.', 1)[0]
        
        # ç§»é™¤éšæœºå‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if '_' in name_without_ext:
            name_without_ext = name_without_ext.split('_', 1)[1]
        
        # æ¢å¤URLä¸­çš„æ–œæ 
        original_url = name_without_ext.replace('$', '/')
        
        logger.info(f"ä»æ–‡ä»¶åè§£æå‡ºåŸå§‹URL: {original_url}")
        return {
            'original_url': original_url
        }
    except Exception as e:
        logger.error(f"è§£ææ–‡ä»¶åå¤±è´¥: {str(e)}")
        return {
            'original_url': ''
        }

class WebClipperHandler:
    def __init__(self, config):
        self.config = config
        self.github_client = Github(config['github_token'])
        self.notion_client = Client(auth=config['notion_token'])
        self.telegram_bot = telegram.Bot(token=config['telegram_token'])
        
        # é…ç½® AI æœåŠ¡
        self.ai_provider = config.get('ai_provider', 'openai').lower()
        
        if self.ai_provider == 'azure':
            # Azure OpenAI é…ç½®
            openai.api_type = "azure"
            openai.api_key = config['azure_api_key']
            openai.api_base = config['azure_api_base']
            openai.api_version = config.get('azure_api_version', '2024-02-15-preview')
            logger.info(f"ä½¿ç”¨ Azure OpenAI API: {config['azure_api_base']}")
        else:
            # æ ‡å‡† OpenAI é…ç½®
            openai.api_key = config['openai_api_key']
            if 'openai_base_url' in config:
                openai.base_url = config['openai_base_url']
                logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰ OpenAI API URL: {config['openai_base_url']}")

    async def process_file(self, file_path: Path, original_url: str = ''):
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
        try:
            logger.info("ğŸ”„ å¼€å§‹å¤„ç†æ–°çš„ç½‘é¡µå‰ªè—...")
            
            # 1. ä¸Šä¼ åˆ° GitHub Pages
            filename, github_url = self.upload_to_github(str(file_path))
            logger.info(f"ğŸ“¤ GitHub ä¸Šä¼ æˆåŠŸ: {github_url}")

            # Github URL è½¬æ¢ä¸º Markdown
            md_content = self.url2md(github_url)
            
            # 2. è·å–é¡µé¢æ ‡é¢˜
            title = self.get_page_content_by_md(md_content)
            logger.info(f"ğŸ“‘ é¡µé¢æ ‡é¢˜: {title}")
            
            # å¦‚æœæ²¡æœ‰æä¾›åŸå§‹ URLï¼Œåˆ™ä»æ–‡ä»¶åè§£æ
            if not original_url:
                file_info = parse_filename(filename)
                original_url = file_info['original_url']
            
            # 3. ç”Ÿæˆæ‘˜è¦å’Œæ ‡ç­¾
            summary, tags = self.generate_summary_tags(md_content)
            logger.info(f"ğŸ“ æ‘˜è¦: {summary[:100]}...")
            logger.info(f"ğŸ·ï¸ æ ‡ç­¾: {', '.join(tags)}")
            
            # 4. ä¿å­˜åˆ° Notion
            notion_url = self.save_to_notion({
                'title': title,
                'original_url': original_url,
                'snapshot_url': github_url,
                'summary': summary,
                'tags': tags,
                'created_at': time.time()
            })
            logger.info(f"ğŸ““ Notion ä¿å­˜æˆåŠŸ")
            
            # 5. å‘é€ Telegram é€šçŸ¥
            notification = (
                f"âœ¨ æ–°çš„ç½‘é¡µå‰ªè—\n\n"
                f"ğŸ“‘ {title}\n\n"
                f"ğŸ“ {summary}\n\n"
                f"ğŸ”— åŸå§‹é“¾æ¥ï¼š{original_url}\n"
                f"ğŸ“š å¿«ç…§é“¾æ¥ï¼š{github_url}"
            )
            await self.send_telegram_notification(notification)
            
            logger.info("=" * 50)
            logger.info("âœ¨ ç½‘é¡µå‰ªè—å¤„ç†å®Œæˆ!")
            logger.info(f"ğŸ“ åŸå§‹é“¾æ¥: {original_url}")
            logger.info(f"ğŸ”— GitHubé¢„è§ˆ: {github_url}")
            logger.info(f"ğŸ“š Notionç¬”è®°: {notion_url}")
            logger.info("=" * 50)
            
            return {
                "status": "success",
                "github_url": github_url,
                "notion_url": notion_url
            }
            
        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            logger.error("=" * 50)
            await self.send_telegram_notification(error_msg)
            raise

    def upload_to_github(self, html_path):
        """ä¸Šä¼  HTML æ–‡ä»¶åˆ° GitHub Pages"""
        filename = os.path.basename(html_path)
        max_retries = 5
        retry_delay = 3  # ç§’
        
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        for attempt in range(max_retries):
            try:
                repo = self.github_client.get_repo(self.config['github_repo'])
                file_path = f"clips/{filename}"
                
                try:
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                    existing_file = repo.get_contents(file_path)
                    logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ›´æ–°å†…å®¹: {file_path}")
                    repo.update_file(
                        file_path,
                        f"Update web clip: {filename}",
                        content,
                        existing_file.sha,
                        branch="main"
                    )
                except Exception:
                    logger.info(f"åˆ›å»ºæ–°æ–‡ä»¶: {file_path}")
                    repo.create_file(
                        file_path,
                        f"Add web clip: {filename}",
                        content,
                        branch="main"
                    )
                
                github_url = f"https://{self.config['github_pages_domain']}/{self.config['github_repo'].split('/')[1]}/clips/{filename}"
                logger.info(f"ğŸ“‘ GitHub URL: {github_url}")
                
                # ç­‰å¾… GitHub Pages éƒ¨ç½²
                max_deploy_retries = self.config.get('github_pages_max_retries', 60)
                deploy_retry_interval = 5  # ç§’
                
                logger.info(f"ç­‰å¾… GitHub Pages éƒ¨ç½² (æœ€å¤š {max_deploy_retries * deploy_retry_interval} ç§’)...")
                start_time = time.time()
                
                for deploy_attempt in range(max_deploy_retries):
                    try:
                        response = requests.get(
                            github_url,
                            timeout=10,
                            verify=True,
                            headers={'Cache-Control': 'no-cache'}
                        )
                        
                        if response.status_code == 200:
                            elapsed_time = time.time() - start_time
                            logger.info(f"âœ… GitHub Pages éƒ¨ç½²å®Œæˆ! è€—æ—¶: {elapsed_time:.1f} ç§’")
                            return filename, github_url
                        
                        if deploy_attempt % 6 == 0:  # æ¯30ç§’è¾“å‡ºä¸€æ¬¡ç­‰å¾…ä¿¡æ¯
                            elapsed_time = time.time() - start_time
                            logger.info(f"â³ æ­£åœ¨ç­‰å¾…éƒ¨ç½²... ({elapsed_time:.1f} ç§’)")
                        
                        time.sleep(deploy_retry_interval)
                        
                    except requests.RequestException as e:
                        if deploy_attempt % 6 == 0:
                            logger.warning(f"éƒ¨ç½²æ£€æŸ¥å¤±è´¥ ({deploy_attempt + 1}/{max_deploy_retries}): {str(e)}")
                        time.sleep(deploy_retry_interval)
                        continue
                
                logger.warning("âš ï¸ GitHub Pages éƒ¨ç½²è¶…æ—¶ï¼Œä½†ç»§ç»­å¤„ç†...")
                return filename, github_url
                
            except Exception as e:
                logger.warning(f"GitHub ä¸Šä¼ å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    logger.error(f"âŒ GitHub ä¸Šä¼ æœ€ç»ˆå¤±è´¥: {str(e)}")
                    raise

    def url2md(self, url, max_retries=30):
        """å°† URL è½¬æ¢ä¸º Markdown"""
        try:
            for attempt in range(max_retries):
                try:
                    md_url = f"https://r.jina.ai/{url}"
                    response = requests.get(md_url)
                    if response.status_code == 200:
                        md_content = response.text
                        return md_content
                except Exception:
                    time.sleep(10)
        except Exception:
            md_content = self.get_page_content_by_bs(url)
            return md_content

    def generate_summary_tags(self, content):
        """ä½¿ç”¨ AI ç”Ÿæˆæ‘˜è¦å’Œæ ‡ç­¾"""
        try:
            messages = [{
                "role": "user",
                "content": """è¯·ä¸ºä»¥ä¸‹ç½‘é¡µå†…å®¹ç”Ÿæˆç®€çŸ­æ‘˜è¦å’Œç›¸å…³æ ‡ç­¾ã€‚

è¦æ±‚ï¼š
1. æ— è®ºåŸæ–‡æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡ï¼Œéƒ½å¿…é¡»ç”¨ä¸­æ–‡å›å¤
2. æ‘˜è¦æ§åˆ¶åœ¨100å­—ä»¥å†…
3. ç”Ÿæˆ3-5ä¸ªä¸­æ–‡æ ‡ç­¾
4. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š

æ‘˜è¦ï¼š[100å­—ä»¥å†…çš„ä¸­æ–‡æ‘˜è¦]
æ ‡ç­¾ï¼štag1ï¼Œtag2ï¼Œtag3ï¼Œtag4ï¼Œtag5

ç½‘é¡µå†…å®¹ï¼š
""" + content[:5000] + "..."
            }]

            if self.ai_provider == 'azure':
                client = openai.AzureOpenAI(
                    api_key=self.config['azure_api_key'],
                    api_version=self.config.get('azure_api_version', '2024-02-15-preview'),
                    azure_endpoint=self.config['azure_api_base']
                )
                response = client.chat.completions.create(
                    model=self.config['azure_deployment_name'],
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1000
                )
            else:
                client = openai.OpenAI(
                    api_key=self.config['openai_api_key'],
                    base_url=self.config.get('openai_base_url')
                )
                response = client.chat.completions.create(
                    model=self.config.get('openai_model', 'gpt-3.5-turbo'),
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1000
                )

            result = response.choices[0].message.content
            
            try:
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„è§£æé€»è¾‘
                parts = result.split('\n')
                summary_part = next(p for p in parts if 'æ‘˜è¦ï¼š' in p)
                tags_part = next(p for p in parts if 'æ ‡ç­¾ï¼š' in p)
                
                summary = summary_part.split('æ‘˜è¦ï¼š', 1)[1].strip()
                tags_str = tags_part.split('æ ‡ç­¾ï¼š', 1)[1].strip()
                
                # å¤„ç†æ ‡ç­¾
                tags = [
                    tag.strip()
                    for tag in tags_str.replace('ï¼Œ', ',').split(',')
                    if tag.strip() and len(tag.strip()) <= 20  # é™åˆ¶æ ‡ç­¾é•¿åº¦
                ]
                
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ ‡ç­¾
                if not tags:
                    tags = ["æœªåˆ†ç±»"]
                
                # è®°å½•ç”Ÿæˆçš„ç»“æœ
                logger.info("AI ç”Ÿæˆç»“æœ:")
                logger.info(f"æ‘˜è¦: {summary}")
                logger.info(f"æ ‡ç­¾: {', '.join(tags)}")
                
                return summary, tags
                
            except Exception as e:
                logger.error(f"è§£æ AI å“åº”å¤±è´¥: {str(e)}")
                logger.error(f"AI åŸå§‹å“åº”: {result}")
                return "æ— æ³•è§£ææ‘˜è¦", ["æœªåˆ†ç±»"]
            
        except Exception as e:
            logger.error(f"OpenAI API è°ƒç”¨å¤±è´¥: {str(e)}")
            if hasattr(e, 'response'):
                logger.error(f"API å“åº”: {e.response}")
            return "æ— æ³•ç”Ÿæˆæ‘˜è¦", ["æœªåˆ†ç±»"]

    def save_to_notion(self, data):
        """ä¿å­˜åˆ° Notion æ•°æ®åº“"""
        try:
            tags = data.get('tags', [])
            if not tags:
                tags = ["æœªåˆ†ç±»"]
            
            current_time = time.strftime('%Y-%m-%dT%H:%M:%S.000Z', 
                                       time.gmtime(data['created_at']))
            
            properties = {
                "Title": {"title": [{"text": {"content": data['title']}}]},
                "OriginalURL": {"url": data['original_url'] if data['original_url'] else None},
                "SnapshotURL": {"url": data['snapshot_url']},
                "Summary": {"rich_text": [{"text": {"content": data['summary']}}]},
                "Tags": {"multi_select": [{"name": tag} for tag in tags if tag.strip()]},
                "Created": {"date": {"start": current_time}}
            }
            
            response = self.notion_client.pages.create(
                parent={"database_id": self.config['notion_database_id']},
                properties=properties
            )
            
            return response['url']
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ° Notion å¤±è´¥: {str(e)}")
            if hasattr(e, 'response'):
                logger.error(f"Notion API å“åº”: {e.response.text}")
            raise

    def get_page_content_by_md(self, md_content):
        """ä» markdown è·å–æ ‡é¢˜"""
        lines = md_content.splitlines()
        for line in lines:
            if line.startswith("Title:"):
                return line.replace("Title:", "").strip()
        return "æœªçŸ¥æ ‡é¢˜"

    def get_page_content_by_bs(self, url, max_retries=60):
        """ä»éƒ¨ç½²çš„é¡µé¢è·å–æ ‡é¢˜å’Œå†…å®¹"""
        for attempt in range(max_retries):
            try:
                response = requests.get(url)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # è·å–æ ‡é¢˜
                    title = None
                    if soup.title:
                        title = soup.title.string
                    if not title and soup.h1:
                        title = soup.h1.get_text(strip=True)
                    if not title:
                        for tag in ['h2', 'h3', 'h4', 'h5', 'h6']:
                            if soup.find(tag):
                                title = soup.find(tag).get_text(strip=True)
                                break
                    
                    # æ¸…ç†æ ‡é¢˜
                    # if title:
                    #     title = ' '.join(title.split())
                    #     title = re.sub(r'\s*[-|]\s*.*$', '', title)
                    # else:
                    #     title = os.path.basename(url)
                    
                    # æå–æ­£æ–‡å†…å®¹
                    # for script in soup(["script", "style"]):
                    #     script.decompose()
                    
                    # text = soup.get_text()
                    # lines = (line.strip() for line in text.splitlines())
                    # chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    # text = ' '.join(chunk for chunk in chunks if chunk)

                    # æå–æ­£æ–‡å†…å®¹
                    html2markdown = html2text.HTML2Text()
                    html2markdown.ignore_links = True
                    html2markdown.ignore_images = True
                    content = html2markdown.handle(soup.prettify())
                    
                    return f"Title: {title} \n\n {content}"
                    
                time.sleep(5)
                
            except Exception:
                time.sleep(5)
        
        return os.path.basename(url), ""

    async def send_telegram_notification(self, message):
        """å‘é€ Telegram é€šçŸ¥"""
        await self.telegram_bot.send_message(
            chat_id=self.config['telegram_chat_id'],
            text=message
        )

@app.post("/")  # æ”¯æŒæ ¹è·¯å¾„
@app.post("/upload")  # æ”¯æŒä¸å¸¦æ–œæ çš„ /upload
@app.post("/upload/")  # ä¿æŒåŸæœ‰çš„ /upload/
@limiter.limit("10/minute", key_func=get_remote_address)
async def upload_file(
    request: Request,
    token: str = Depends(verify_token)
):
    """æ–‡ä»¶ä¸Šä¼ æ¥å£"""
    try:
        form = await request.form()
        original_url = form.get('url', '')
        
        # è·å–æ–‡ä»¶å†…å®¹
        file = None
        for field_name, field_value in form.items():
            if hasattr(field_value, 'filename') and hasattr(field_value, 'read'):
                file = field_value
                break
        
        if not file:
            raise HTTPException(
                status_code=400,
                detail="No file content found in form data"
            )
        
        filename = file.filename
        content = await file.read()
        
        # éªŒè¯å’Œä¿å­˜æ–‡ä»¶
        file_ext = Path(filename).suffix.lower()
        if not file_ext:
            filename += '.html'
        elif file_ext not in ALLOWED_EXTENSIONS:
            raise HTTPException(
                status_code=400,
                detail=f"File type not allowed. Allowed types: {', '.join(ALLOWED_EXTENSIONS)}"
            )
        
        if len(content) > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=400,
                detail=f"File too large. Maximum size allowed: {MAX_FILE_SIZE/1024/1024}MB"
            )
        
        # ä¿å­˜æ–‡ä»¶
        safe_filename = f"{secrets.token_hex(8)}_{filename}"
        file_path = UPLOAD_DIR / safe_filename
        
        with open(file_path, "wb") as f:
            f.write(content)
        
        try:
            result = await handler.process_file(file_path, original_url)
            return result
        finally:
            if file_path.exists():
                file_path.unlink()
                
    except HTTPException:
        raise
    except Exception as e:
        error_msg = f"ä¸Šä¼ å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        raise HTTPException(
            status_code=500,
            detail=str(e)
        )

def start_server(host="0.0.0.0", port=8000):
    """å¯åŠ¨æœåŠ¡å™¨"""
    logger.info(f"Starting server on {host}:{port}")
    logger.info(f"Upload endpoints: /, /upload, /upload/")
    logger.info(f"API Key required in Bearer token")
    uvicorn.run(app, host=host, port=port)